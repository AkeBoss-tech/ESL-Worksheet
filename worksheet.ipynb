{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from util import generate_fill_worksheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couple/file-2.txt\n",
      "level/file-6-71.txt\n",
      "level/file-6-89.txt\n",
      "couple/file-17.txt\n",
      "people/file-20.txt\n",
      "essays/file-3.txt\n",
      "essays/file-50.txt\n",
      "essays/file-49.txt\n",
      "customs/file-51.txt\n",
      "essays/file-18.txt\n",
      "level/file-5-99.txt\n",
      "essays/file-3.txt\n",
      "customs/file-93.txt\n",
      "people/file-11.txt\n",
      "couple/file-21.txt\n",
      "couple/file-44.txt\n",
      "people/file-67.txt\n",
      "people/file-64.txt\n",
      "level/file-3-99.txt\n",
      "people/file-83.txt\n",
      "essays/file-86.txt\n",
      "essays/file-45.txt\n",
      "people/file-10.txt\n",
      "couple/file-52.txt\n",
      "couple/file-14.txt\n",
      "essays/file-77.txt\n",
      "couple/file-28.txt\n",
      "customs/file-13.txt\n",
      "people/file-57.txt\n",
      "level/file-4-86.txt\n",
      "\tFill in the Verb\n",
      "\n",
      "1. I _____ you should drop out of the Homecoming race. (to think)\n",
      "2. Voice ________ happens to everyone. (to crack)\n",
      "3. He ___ coughing and sneezing at work. (to be)\n",
      "4. It ________ the president of the United States, the vice-president, and the cabinet. (to include)\n",
      "5. His myth states that he _______ fire to the world. (to bring)\n",
      "6. It is located on Liberty Island _____ off the southern tip of Manhattan, New York City. (to right)\n",
      "7. A part of the avenue ________ expensive designer stores like Tiffany's. (to feature)\n",
      "8. The winner of the sweepstakes ___ going to be announced on the Herrio Cereal website. (to be)\n",
      "9. In 1993 the first of several people ____ forward to accuse Jackson of sexually improper conduct on a child. (to come)\n",
      "10. Mike ______ on the television. (to turn)\n",
      "11. Mary and Maria ___ attended high school together. (to have)\n",
      "12. This site at Stanford would ______ to the Google search engine today. (to evolve)\n",
      "13. She also ______ for Stride Rite, and Hasbro Corporations. (to work)\n",
      "14. Some people _____ the rise of helicopter parenting on the rise of cell phones. (to blame)\n",
      "15. He ___ voted Forbes Magazine's Most Influential Celebrity in America in 2014. (to be)\n",
      "16. Little by little, they ______ the theater. (to exit)\n",
      "17. In the times before the invention of the personal computer, and the advent of the World Wide Web, social networking ___ done in person. (to be)\n",
      "18. Volunteers _____ U.S. (to place)\n",
      "19. In 1920, she ______ found the American Civil Liberties Union, which is still active today. (to help)\n",
      "20. They ______. (to talk)\n",
      "\n",
      "\n",
      "\tAnswer Key\n",
      "\n",
      "1. think\n",
      "2. cracking\n",
      "3. was\n",
      "4. includes\n",
      "5. brought\n",
      "6. right\n",
      "7. features\n",
      "8. was\n",
      "9. came\n",
      "10. turned\n",
      "11. had\n",
      "12. evolve\n",
      "13. worked\n",
      "14. blame\n",
      "15. was\n",
      "16. exited\n",
      "17. was\n",
      "18. place\n",
      "19. helped\n",
      "20. talked\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\tFill in the Verb\\n\\n1. I _____ you should drop out of the Homecoming race. (to think)\\n2. Voice ________ happens to everyone. (to crack)\\n3. He ___ coughing and sneezing at work. (to be)\\n4. It ________ the president of the United States, the vice-president, and the cabinet. (to include)\\n5. His myth states that he _______ fire to the world. (to bring)\\n6. It is located on Liberty Island _____ off the southern tip of Manhattan, New York City. (to right)\\n7. A part of the avenue ________ expensive designer stores like Tiffany's. (to feature)\\n8. The winner of the sweepstakes ___ going to be announced on the Herrio Cereal website. (to be)\\n9. In 1993 the first of several people ____ forward to accuse Jackson of sexually improper conduct on a child. (to come)\\n10. Mike ______ on the television. (to turn)\\n11. Mary and Maria ___ attended high school together. (to have)\\n12. This site at Stanford would ______ to the Google search engine today. (to evolve)\\n13. She also ______ for Stride Rite, and Hasbro Corporations. (to work)\\n14. Some people _____ the rise of helicopter parenting on the rise of cell phones. (to blame)\\n15. He ___ voted Forbes Magazine's Most Influential Celebrity in America in 2014. (to be)\\n16. Little by little, they ______ the theater. (to exit)\\n17. In the times before the invention of the personal computer, and the advent of the World Wide Web, social networking ___ done in person. (to be)\\n18. Volunteers _____ U.S. (to place)\\n19. In 1920, she ______ found the American Civil Liberties Union, which is still active today. (to help)\\n20. They ______. (to talk)\\n\\n\\n\\tAnswer Key\\n\\n1. think\\n2. cracking\\n3. was\\n4. includes\\n5. brought\\n6. right\\n7. features\\n8. was\\n9. came\\n10. turned\\n11. had\\n12. evolve\\n13. worked\\n14. blame\\n15. was\\n16. exited\\n17. was\\n18. place\\n19. helped\\n20. talked\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_fill_worksheet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from util import is_verb\n",
    "\n",
    "bank = set()\n",
    "\n",
    "# similar_words = wordnet.similar_tos('run')\n",
    "for syn in wordnet.synsets(\"run\"):\n",
    "    if not is_verb(syn.name().split('.')[0]): continue\n",
    "\n",
    "    for lemma in syn.lemmas():\n",
    "        bank.add(lemma.name())\n",
    "\n",
    "bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level/file-1-67.txt\n",
      "['He hears a loud noise. He looks out the window. He sees a motorcycle. It is on the road. It is black and silver. It has two wheels. There is a man on the motorcycle. He wears a helmet. He wears sunglasses. He wears a leather jacket. He is riding the motorcycle. ']\n",
      "1. motorcycle - a motor vehicle with two wheels and a strong frame\n",
      "2. sunglasses - spectacles that are darkened or polarized to protect the eyes from the glare of the sun\n",
      "3. wears - impairment resulting from long use\n",
      "3. helmet - armor plate that protects the head\n",
      "4. hears - perceive (sound) via the auditory sense\n",
      "\tVocabulary Worksheet\n",
      "\n",
      "1. motorcycle\t\t\ta. spectacles that are darkened or polarized to protect the eyes from the glare of the sun\n",
      "2. sunglasses\t\t\tb. armor plate that protects the head\n",
      "3. helmet\t\t\tc. a motor vehicle with two wheels and a strong frame\n",
      "\n",
      "\n",
      "\tAnswer Key\n",
      "\n",
      "1. motorcycle - a motor vehicle with two wheels and a strong frame\n",
      "2. sunglasses - spectacles that are darkened or polarized to protect the eyes from the glare of the sun\n",
      "3. helmet - armor plate that protects the head\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import util\n",
    "# asks for inputs in notebook\n",
    "worksheet = util.generate_vocab_worksheet_interactive()\n",
    "print(worksheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customs/file-28.txt\n",
      "\tVocabulary Worksheet\n",
      "\n",
      "1. english\t\t\ta. a body of students who are taught together\n",
      "2. finishes\t\t\tb. move forward, also in the metaphorical sense\n",
      "3. lunches\t\t\tc. an opening that permits escape or release\n",
      "4. tuitions\t\t\td. coming next after the eleventh and just before the thirteenth in position\n",
      "5. standardized\t\t\te. cause to conform to standard or norm\n",
      "6. u.s.\t\t\tf. the executive and legislative and judicial branches of the federal government of the United States\n",
      "7. diploma\t\t\tg. a fee paid for instruction (especially for higher education)\n",
      "8. completing\t\t\th. a course that the student can select from among alternatives\n",
      "9. textbooks\t\t\ti. a set of questions or exercises evaluating skill or knowledge\n",
      "10. electives\t\t\tj. a midday meal\n",
      "11. exam\t\t\tk. a document certifying the successful completion of a course of study\n",
      "12. specialized\t\t\tl. come or bring to a finish or an end\n",
      "13. graduation\t\t\tm. become more focus on an area of activity or field of study\n",
      "14. grades\t\t\tn. an Indo-European language belonging to the West Germanic branch; the official language of Britain and the United States and most of the commonwealth countries\n",
      "15. advanced\t\t\to. the successful completion of a program of study\n",
      "16. exit\t\t\tp. a book prepared for use in schools or colleges\n",
      "17. united\t\t\tq. a decorative texture or appearance of a surface (or the substance that gives it that appearance)\n",
      "18. 12th\t\t\tr. act in concert or unite in a common purpose or belief\n",
      "19. pass\t\t\ts. (baseball) an advance to first base by a batter who receives four balls\n",
      "\n",
      "\n",
      "\tAnswer Key\n",
      "\n",
      "1. english - an Indo-European language belonging to the West Germanic branch; the official language of Britain and the United States and most of the commonwealth countries\n",
      "2. finishes - a decorative texture or appearance of a surface (or the substance that gives it that appearance)\n",
      "3. lunches - a midday meal\n",
      "4. tuitions - a fee paid for instruction (especially for higher education)\n",
      "5. standardized - cause to conform to standard or norm\n",
      "6. u.s. - the executive and legislative and judicial branches of the federal government of the United States\n",
      "7. diploma - a document certifying the successful completion of a course of study\n",
      "8. completing - come or bring to a finish or an end\n",
      "9. textbooks - a book prepared for use in schools or colleges\n",
      "10. electives - a course that the student can select from among alternatives\n",
      "11. exam - a set of questions or exercises evaluating skill or knowledge\n",
      "12. specialized - become more focus on an area of activity or field of study\n",
      "13. graduation - the successful completion of a program of study\n",
      "14. grades - a body of students who are taught together\n",
      "15. advanced - move forward, also in the metaphorical sense\n",
      "16. exit - an opening that permits escape or release\n",
      "17. united - act in concert or unite in a common purpose or belief\n",
      "18. 12th - coming next after the eleventh and just before the thirteenth in position\n",
      "19. pass - (baseball) an advance to first base by a batter who receives four balls\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import util\n",
    "# doesn't ask for inputs in notebook\n",
    "worksheet = util.generate_vocab_worksheet()\n",
    "print(worksheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level/file-3-17.txt\n",
      "couple/file-60.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people/file-48.txt\n",
      "['signaled']\n",
      "['was']\n",
      "['starred']\n",
      "['was', 'was']\n",
      "['had']\n",
      "['was', 'weighed', 'made']\n",
      "['won', 'was', 'were']\n",
      "['went', 'had']\n",
      "['were']\n",
      "['was']\n",
      "['stopped']\n",
      "['suffered', 'lost']\n",
      "['refused']\n",
      "['spent']\n",
      "['used', 'died']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['It signaled the beginning of a new episode of The Adventures of Superman',\n",
       " 'The 1960s show is just one in a series of shows, plays, movies, and comic books that depict the \"Man of Steel.\" The most iconic actor portraying Superman was Christopher Reeve',\n",
       " 'Reeve starred in four movies during the late 1970s and 80s',\n",
       " 'In the 1978 movie classic Superman, Reeve was cast as the legendary American hero based largely on his physical attributes, although he was an accomplished actor',\n",
       " 'Reeve had the classic Superman look',\n",
       " 'He was 6\\'4\" tall, weighed 215 lbs., with a square jaw, and other manly features that made him perfect for the role',\n",
       " 'The film won critical acclaim, and multiple awards, including a \"Special Achievement Award for Visual Effects.\" One of the selling points for the movie was that the special effects were so good that audience would believe that a man can fly after seeing this film',\n",
       " 'Reeve went on to star in three sequels to the original movie, and had a very solid acting career in several movies after that',\n",
       " 'Tragically, Reeve and his family were hit with a devastating blow on May 27, 1995',\n",
       " 'Reeve was riding his horse through a steeplechase course jumping over obstacles',\n",
       " 'His horse suddenly stopped, throwing Reeve to the ground',\n",
       " 'Reeve suffered shattered vertebrae, and instantly lost the use of his arms and legs',\n",
       " 'Reeve refused to give up',\n",
       " 'He spent the rest of his years advocating for spinal cord injury advancement in treatment and technology',\n",
       " 'He used his celebrity status to help the cause, but sadly, Reeve died at the age of 52 in 2004 due to complications from his injuries']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import util\n",
    "util.get_sentences_with_verb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essays/file-3.txt\n",
      "couple/file-49.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from util import generate_worksheets_for_verb, random_story\n",
    "generate_worksheets_for_verb(random_story(), verb_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level/file-2-54.txt\n",
      "customs/file-71.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from util import random_story, generate_worksheets_for_verb, generate_vocab_worksheet\n",
    "story = random_story()\n",
    "full_worksheet = '# Worksheet\\n\\n'\n",
    "for paragraph in story:\n",
    "    full_worksheet += '\\t' + paragraph + '\\n'\n",
    "\n",
    "full_worksheet += '\\n\\n'\n",
    "\n",
    "# add worksheets for two verb types\n",
    "for i in range(5):\n",
    "    full_worksheet += generate_worksheets_for_verb(story, verb_type=i)\n",
    "    full_worksheet += '\\n\\n'\n",
    "\n",
    "# add definition worksheet\n",
    "full_worksheet += generate_vocab_worksheet(story)\n",
    "\n",
    "print(full_worksheet)\n",
    "\n",
    "# store it in a file\n",
    "with open('worksheet.md', 'w') as f:\n",
    "    f.write(full_worksheet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
