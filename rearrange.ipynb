{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import choice\n",
    "\n",
    "# load random story\n",
    "def random_story(folder=None):\n",
    "    if folder is None:\n",
    "        folder = choice(os.listdir('ESL-stuff'))\n",
    "        \n",
    "    story = choice(os.listdir('ESL-stuff/' + folder))\n",
    "    print(folder + '/' + story)\n",
    "    with open('ESL-stuff/' + folder + '/' + story, 'r') as f:\n",
    "        story = f.readlines()\n",
    "        # get rid of empty lines\n",
    "        story = [line for line in story if line != '\\n']\n",
    "    return story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_sentence(story):\n",
    "    \"\"\"Get a random sentence from a story.\"\"\"\n",
    "    paragraph = choice(story).split('. ')\n",
    "    sentence = choice(paragraph)\n",
    "    if len(sentence) < 4: return get_random_sentence(story)\n",
    "\n",
    "    if sentence[0].islower(): return get_random_sentence(story)\n",
    "    \n",
    "    return sentence + \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level/file-3-24.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Kenneth Baker woke up early Sunday morning. A terrible thing had happened. His cat was missing. \"He must have run away again,\" thought Kenneth. He went around asking neighbors for information. \"That\\'s odd,\" said his neighbor Sandra. \"My Chihuahua went missing last night, too!\" Kenneth kept walking around the neighborhood. He hoped to find his cat roaming the streets. Instead, Kenneth found something else. There were signs everywhere. They all said, \"Missing cat\" or \"Missing Dog\". \"I don\\'t think all these animals are running away,\" thought Kenneth. Then, Kenneth saw something that broke his heart. There was a trail of blood on the floor. It led to the mountain side. A coyote must be taking all the small pets in the night. ']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story = random_story(\"level\")\n",
    "story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"He must have run away again,\" thought Kenneth.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_random_sentence(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find verb in sentence\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def find_verb(sentence):\n",
    "    verbs = []\n",
    "    pos_tags = pos_tag(word_tokenize(sentence))\n",
    "    print(pos_tags)\n",
    "    for word, tag in pos_tags:\n",
    "        if tag.startswith('VBD') or tag.startswith('VBP'):\n",
    "            verbs.append(word)\n",
    "\n",
    "    if len(verbs) == 0:\n",
    "        for word, tag in pos_tags:\n",
    "            if tag.startswith('VBZ') or tag.startswith('VB'):\n",
    "                verbs.append(word)\n",
    "    return verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('brown')\n",
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def create_similar_word_bank(words, num=4):\n",
    "    bank = set()\n",
    "\n",
    "    for word in words:\n",
    "        # similar_words = wordnet.similar_tos('dog')\n",
    "        for syn in wordnet.synsets(word):\n",
    "            bank.add(syn.lemmas()[0].name())\n",
    "\n",
    "        \"\"\" for word in similar_words:\n",
    "            bank.append(word) \"\"\"\n",
    "        # bank.append(text.similar(word, num=num))\n",
    "\n",
    "    return bank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level/file-6-72.txt\n",
      "[('Laura', 'NNP'), ('put', 'VBD'), ('the', 'DT'), ('book', 'NN'), ('back', 'RB'), ('in', 'IN'), ('her', 'PRP$'), ('bookshelf', 'NN'), ('and', 'CC'), ('never', 'RB'), ('touched', 'VBD'), ('it', 'PRP'), ('again', 'RB'), ('.', '.')]\n",
      "Similar Verbs {'put_option', 'refer', 'place', 'touch', 'affect', 'reach', 'equal', 'tint', 'arrange', 'touched', 'fey', 'put', 'partake', 'invest', 'allude', 'moved', 'frame'}\n",
      "['Laura ___ the book back in her bookshelf and never touched it again.', 'Laura put the book back in her bookshelf and never _______ it again.']\n"
     ]
    }
   ],
   "source": [
    "sentence = get_random_sentence(random_story())\n",
    "verbs = find_verb(sentence)\n",
    "print(\"Similar Verbs\", create_similar_word_bank(verbs))\n",
    "new_sentence = [sentence.replace(verb, \"_\"*len(verb), 1) for verb in verbs]\n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CC - coordinating conjunction\n",
    "\n",
    "CD - cardinal number\n",
    "\n",
    "DT - determiner\n",
    "\n",
    "E - existential there (e.g., \"there is\")\n",
    "\n",
    "FW - foreign word\n",
    "\n",
    "IN - preposition or subordinating conjunction\n",
    "\n",
    "JJ - adjective\n",
    "\n",
    "JJR - comparative adjective\n",
    "\n",
    "JJS - superlative adjective\n",
    "\n",
    "LS - list item marker\n",
    "\n",
    "MD - modal verb\n",
    "\n",
    "NN - noun, singular or mass\n",
    "\n",
    "NNS - noun, plural\n",
    "\n",
    "NNP - proper noun, singular\n",
    "\n",
    "NNPS - proper noun, plural\n",
    "\n",
    "PDT - predeterminer\n",
    "\n",
    "POS - possessive pronoun\n",
    "\n",
    "PRP - personal pronoun\n",
    "\n",
    "PRP$ - possessive pronoun\n",
    "\n",
    "RB - adverb\n",
    "\n",
    "RBR - comparative adverb\n",
    "\n",
    "RBS - superlative adverb\n",
    "\n",
    "詞 - Japanese particle\n",
    "\n",
    "SYM - Chinese character\n",
    "\n",
    "TO - infinitive marker\n",
    "\n",
    "UH - interjection\n",
    "\n",
    "VB - infinitive marker\n",
    "\n",
    "VBD - past tense verb\n",
    "\n",
    "VBG - past participle verb\n",
    "\n",
    "VBN - past participle verb\n",
    "\n",
    "VBP - present tense, perfect, and past participle verb\n",
    "\n",
    "VBZ - present tense, present participle verb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_infinitive_form(word):\n",
    "    return \"to \" + lemmatizer.lemmatize(word, pos='v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essays/file-60.txt\n",
      "[('Chicago', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('home', 'NN'), ('of', 'IN'), ('other', 'JJ'), ('famous', 'JJ'), ('firsts', 'NNS'), ('.', '.')]\n",
      "is\n",
      "level/file-3-86.txt\n",
      "[('He', 'PRP'), ('needed', 'VBD'), ('to', 'TO'), ('watch', 'VB'), ('the', 'DT'), ('Baseball', 'NNP'), ('final', 'JJ'), ('.', '.')]\n",
      "needed\n",
      "essays/file-33.txt\n",
      "[('If', 'IN'), ('you', 'PRP'), ('pass', 'VBP'), ('this', 'DT'), ('test', 'NN'), (',', ','), ('you', 'PRP'), ('can', 'MD'), ('practice', 'NN'), ('driving', 'VBG'), ('so', 'IN'), ('you', 'PRP'), ('can', 'MD'), ('pass', 'VB'), ('a', 'DT'), ('road', 'NN'), ('test', 'NN'), ('and', 'CC'), ('get', 'VB'), ('your', 'PRP$'), ('license', 'NN'), ('.', '.')]\n",
      "pass\n",
      "couple/file-9.txt\n",
      "[('He', 'PRP'), ('stumbled', 'VBD'), ('upon', 'RP'), ('a', 'DT'), ('steakhouse', 'NN'), ('located', 'VBN'), ('a', 'DT'), ('few', 'JJ'), ('miles', 'NNS'), ('north', 'RB'), ('of', 'IN'), ('where', 'WRB'), ('they', 'PRP'), ('lived', 'VBD'), ('.', '.')]\n",
      "essays/file-26.txt\n",
      "[('begin', 'VB'), ('school', 'NN'), ('at', 'IN'), ('age', 'NN'), ('5', 'CD'), (',', ','), ('when', 'WRB'), ('they', 'PRP'), ('go', 'VBP'), ('to', 'TO'), ('kindergarten', 'VB'), ('.', '.')]\n",
      "go\n",
      "customs/file-69.txt\n",
      "[('Both', 'DT'), ('the', 'DT'), ('New', 'NNP'), ('York', 'NNP'), ('Philharmonic', 'NNP'), ('Orchestra', 'NNP'), ('and', 'CC'), ('the', 'DT'), ('Metropolitan', 'NNP'), ('Opera', 'NNP'), ('also', 'RB'), ('host', 'VBD'), ('free', 'JJ'), ('performances', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('park', 'NN'), ('in', 'IN'), ('the', 'DT'), ('spring', 'NN'), ('and', 'CC'), ('summer', 'NN'), ('.', '.')]\n",
      "host\n",
      "people/file-68.txt\n",
      "[('He', 'PRP'), ('was', 'VBD'), ('hired', 'VBN'), ('by', 'IN'), ('Barber', 'NNP'), (',', ','), ('who', 'WP'), ('served', 'VBD'), ('as', 'IN'), ('mentor', 'NN'), ('for', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('broadcaster', 'NN'), ('.', '.')]\n",
      "essays/file-87.txt\n",
      "[('Having', 'VBG'), ('your', 'PRP$'), ('kid', 'NN'), ('physically', 'RB'), ('attached', 'VBN'), ('to', 'TO'), ('you', 'PRP'), ('probably', 'RB'), ('does', 'VBZ'), (\"n't\", 'RB'), ('sound', 'VB'), ('like', 'IN'), ('fun', 'NN'), (',', ','), ('but', 'CC'), ('an', 'DT'), ('American', 'JJ'), ('pediatrician', 'NN'), (',', ','), ('William', 'NNP'), ('Sears', 'NNP'), (',', ','), ('promotes', 'VBZ'), ('a', 'DT'), ('style', 'NN'), ('of', 'IN'), ('parenting', 'VBG'), ('that', 'IN'), ('keeps', 'VBZ'), ('small', 'JJ'), ('children', 'NNS'), ('as', 'IN'), ('physically', 'RB'), ('attached', 'VBN'), ('as', 'IN'), ('possible', 'JJ'), ('.', '.')]\n",
      "people/file-18.txt\n",
      "[('He', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('historic', 'JJ'), ('figure', 'NN'), ('primarily', 'RB'), ('because', 'IN'), ('he', 'PRP'), ('is', 'VBZ'), ('the', 'DT'), ('first', 'JJ'), ('African', 'JJ'), ('American', 'NNP'), ('to', 'TO'), ('hold', 'VB'), ('the', 'DT'), ('position', 'NN'), ('of', 'IN'), ('president', 'NN'), ('.', '.')]\n",
      "level/file-3-10.txt\n",
      "[('She', 'PRP'), ('would', 'MD'), ('make', 'VB'), ('sure', 'JJ'), ('no', 'DT'), ('one', 'NN'), ('was', 'VBD'), ('looking', 'VBG'), ('at', 'IN'), ('her', 'PRP'), ('.', '.')]\n",
      "was\n",
      "level/file-2-8.txt\n",
      "[('He', 'PRP'), ('grabbed', 'VBD'), ('two', 'CD'), ('slices', 'NNS'), ('.', '.')]\n",
      "grabbed\n",
      "people/file-50.txt\n",
      "[('Eastwood', 'NN'), ('was', 'VBD'), ('born', 'VBN'), ('in', 'IN'), ('1930', 'CD'), ('in', 'IN'), ('San', 'NNP'), ('Francisco', 'NNP'), ('before', 'IN'), ('coming', 'VBG'), ('to', 'TO'), ('Los', 'NNP'), ('Angeles', 'NNP'), ('at', 'IN'), ('an', 'DT'), ('early', 'JJ'), ('age', 'NN'), ('.', '.')]\n",
      "was\n",
      "level/file-1-95.txt\n",
      "[('She', 'PRP'), ('gets', 'VBZ'), ('into', 'IN'), ('her', 'PRP$'), ('car', 'NN'), ('.', '.')]\n",
      "gets\n",
      "essays/file-5.txt\n",
      "[('New', 'NNP'), ('Year', 'NNP'), (\"'s\", 'POS'), ('Day', 'NNP'), (',', ','), ('January', 'NNP'), ('1', 'CD'), (',', ','), ('is', 'VBZ'), ('a', 'DT'), ('national', 'JJ'), ('holiday', 'NN'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('.', '.')]\n",
      "is\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "problems = int(input(\"Enter the number of questions\"))\n",
    "while len(sentences) < problems:\n",
    "    random_sentence = get_random_sentence(random_story())\n",
    "    verbs = find_verb(random_sentence)\n",
    "\n",
    "    if len(verbs) > 1 or len(verbs) == 0: continue\n",
    "\n",
    "    new = random_sentence.replace(verbs[0], \"_\"*len(verbs[0]), 1)\n",
    "    print(verbs[0])\n",
    "\n",
    "    if new not in sentences:\n",
    "        sentences.append([new + f\" ({get_infinitive_form(verbs[0])})\", verbs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFill in the Verb\n",
      "\n",
      "1. Chicago __ the home of other famous firsts. (to be)\n",
      "2. He ______ to watch the Baseball final. (to need)\n",
      "3. If you ____ this test, you can practice driving so you can pass a road test and get your license. (to pass)\n",
      "4.  begin school at age 5, when they __ to kindergarten. (to go)\n",
      "5. Both the New York Philharmonic Orchestra and the Metropolitan Opera also ____ free performances in the park in the spring and summer. (to host)\n",
      "6. She would make sure no one ___ looking at her. (to be)\n",
      "7. He _______ two slices. (to grab)\n",
      "8. Eastwood ___ born in 1930 in San Francisco before coming to Los Angeles at an early age. (to be)\n",
      "9. She ____ into her car. (to get)\n",
      "10. New Year's Day, January 1, __ a national holiday in the United States. (to be)\n",
      "\n",
      "\n",
      "\tAnswer Key\n",
      "\n",
      "1. is\n",
      "2. needed\n",
      "3. pass\n",
      "4. go\n",
      "5. host\n",
      "6. was\n",
      "7. grabbed\n",
      "8. was\n",
      "9. gets\n",
      "10. is\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worksheet = \"\\tFill in the Verb\\n\\n\"\n",
    "\n",
    "for index, sentence in enumerate(sentences): \n",
    "    worksheet += f\"{index + 1}. {sentence[0]}\\n\"\n",
    "\n",
    "worksheet += \"\\n\\n\\tAnswer Key\\n\\n\"\n",
    "\n",
    "for index, sentence in enumerate(sentences):\n",
    "    worksheet += f\"{index + 1}. {sentence[1]}\\n\"\n",
    "\n",
    "print(worksheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternate Definitions\n",
      "a native or inhabitant of the United States\n",
      "the English language as used in the United States\n",
      "a native or inhabitant of a North American or Central American or South American country\n",
      "of or relating to the United States of America or its people or language or culture\n",
      "of or relating to or characteristic of the continents and islands of the Americas\n"
     ]
    }
   ],
   "source": [
    "word = \"American\"\n",
    "\n",
    "print(\"Alternate Definitions\")\n",
    "for syn in wordnet.synsets(word):\n",
    "    print(syn.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all complex nouns in sentence\n",
    "# input: sentence\n",
    "# output: list of complex nouns\n",
    "def get_complex_nouns(sentence):\n",
    "    nouns = []\n",
    "    for chunk in sentence.noun_chunks:\n",
    "        if len(chunk.text.split()) > 1:\n",
    "            nouns.append(chunk.text)\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customs/file-17.txt\n",
      "Veterans Day is an official United States holiday observed on November 11th every year. While Memorial Day remembers those who have died while serving in any of the branches of the U.S. Armed Forces -- Army, Navy, Marines, and Coast Guard, Veterans Day honors all members of the armed forces, especially the living. As a U.S. federal holiday, government offices and schools are closed, and most people have the day off from work. Banks are also closed. If November 11th falls on a Sunday, then the holiday is observed on the following Monday. The holiday was first observed after World War I, which ended On November 11 1918. It was called Armistice Day then and was more about celebrating the absence of war and honoring those that served in that one war. It didn't become known as Veterans Day until 1954 after World War II and the Korean War, when there were many more members of the armed forces. Many restaurants offer free meals to veterans on the date. There is always a special ceremony at Arlington National Military Cemetery in Washington D.C. This ceremony involves a wreath being placed over the grave of The Unknown Soldier. Many major cities also hold parades on this date. The U.S. flag is usually flown at half-mast as a remembrance to those soldiers who have died and many ceremonies observe a moment of silence for the same reason. In Canada the holiday is called Remembrance Day. One symbol of the day is the poppy flower. The red poppy flower is said to represent or be a symbol of the blood of soldiers. Many people wear or give out poppy flowers on the day.  \n",
      "Veterans: 20.00\n",
      "Day: 0.48\n",
      "is: 0.03\n",
      "an: 0.01\n",
      "official: 7.67\n",
      "United: 24.00\n",
      "States: 4.15\n",
      "holiday: 11.67\n",
      "observed: 10.67\n",
      "on: 0.02\n",
      "November: 80.00\n",
      "11th: 10.00\n",
      "every: 0.23\n",
      "year: 0.25\n",
      ".: 0.00\n",
      "While: 0.09\n",
      "Memorial: 48.00\n",
      "remembers: 51.43\n",
      "those: 0.00\n",
      "who: 0.01\n",
      "have: 0.21\n",
      "died: 5.06\n",
      "while: 0.09\n",
      "serving: 31.32\n",
      "in: 0.01\n",
      "any: 0.05\n",
      "of: 0.00\n",
      "the: 0.00\n",
      "branches: 18.82\n",
      "U.S.: 80.00\n",
      "Armed: 7.81\n",
      "Forces: 7.65\n",
      "--: 0.00\n",
      "Army: 2.03\n",
      ",: 0.00\n",
      "Navy: 17.14\n",
      "Marines: 19.09\n",
      "and: 0.00\n",
      "Coast: 7.14\n",
      "Guard: 19.44\n",
      "honors: 28.00\n",
      "all: 0.03\n",
      "members: 1.12\n",
      "armed: 7.81\n",
      "forces: 7.65\n",
      "especially: 1.27\n",
      "living: 5.48\n",
      "As: 0.03\n",
      "a: 0.00\n",
      "federal: 3.72\n",
      "government: 1.47\n",
      "offices: 12.25\n",
      "schools: 3.66\n",
      "are: 0.10\n",
      "closed: 14.86\n",
      "most: 0.19\n",
      "people: 0.44\n",
      "day: 0.48\n",
      "off: 0.43\n",
      "from: 0.00\n",
      "work: 1.80\n",
      "Banks: 45.24\n",
      "also: 0.04\n",
      "If: 0.00\n",
      "falls: 95.83\n",
      "Sunday: 120.00\n",
      "then: 0.19\n",
      "following: 13.11\n",
      "Monday: 60.00\n",
      "The: 0.00\n",
      "was: 0.04\n",
      "first: 0.64\n",
      "after: 0.18\n",
      "World: 0.66\n",
      "War: 0.49\n",
      "I: 10.00\n",
      "which: 0.00\n",
      "ended: 4.17\n",
      "On: 0.02\n",
      "11: 0.63\n",
      "1918: 0.00\n",
      "It: 0.00\n",
      "called: 4.23\n",
      "Armistice: 30.00\n",
      "more: 0.09\n",
      "about: 0.23\n",
      "celebrating: 55.00\n",
      "absence: 5.19\n",
      "war: 0.49\n",
      "honoring: 35.56\n",
      "that: 0.00\n",
      "served: 7.50\n",
      "one: 0.09\n",
      "did: 0.39\n",
      "n't: 0.00\n",
      "become: 0.67\n",
      "known: 2.44\n",
      "as: 0.03\n",
      "until: 0.00\n",
      "1954: 0.00\n",
      "II: 40.00\n",
      "Korean: 180.00\n",
      "when: 0.00\n",
      "there: 0.11\n",
      "were: 0.16\n",
      "many: 0.04\n",
      "Many: 0.04\n",
      "restaurants: 8.46\n",
      "offer: 10.00\n",
      "free: 3.61\n",
      "meals: 5.56\n",
      "to: 0.00\n",
      "veterans: 20.00\n",
      "date: 5.05\n",
      "There: 0.11\n",
      "always: 0.67\n",
      "special: 2.99\n",
      "ceremony: 12.63\n",
      "at: 0.01\n",
      "Arlington: 90.00\n",
      "National: 3.00\n",
      "Military: 1.62\n",
      "Cemetery: 6.67\n",
      "Washington: 500.00\n",
      "D.C: 0.00\n",
      "This: 0.00\n",
      "involves: 13.33\n",
      "wreath: 6.67\n",
      "being: 1.08\n",
      "placed: 8.50\n",
      "over: 0.23\n",
      "grave: 11.76\n",
      "Unknown: 11.67\n",
      "Soldier: 5.38\n",
      "major: 2.85\n",
      "cities: 1.68\n",
      "hold: 11.04\n",
      "parades: 87.50\n",
      "this: 0.00\n",
      "flag: 28.24\n",
      "usually: 0.37\n",
      "flown: 140.00\n",
      "half-mast: 90.00\n",
      "remembrance: 73.33\n",
      "soldiers: 4.29\n",
      "ceremonies: 20.00\n",
      "observe: 24.23\n",
      "moment: 1.46\n",
      "silence: 8.75\n",
      "for: 0.00\n",
      "same: 0.35\n",
      "reason: 2.25\n",
      "In: 0.01\n",
      "Canada: 60.00\n",
      "Remembrance: 73.33\n",
      "One: 0.09\n",
      "symbol: 2.18\n",
      "poppy: 16.67\n",
      "flower: 12.63\n",
      "red: 1.71\n",
      "said: 0.25\n",
      "represent: 34.62\n",
      "or: 0.01\n",
      "be: 0.04\n",
      "blood: 2.61\n",
      "wear: 13.33\n",
      "give: 4.74\n",
      "out: 0.25\n",
      "flowers: 5.19\n",
      "\n",
      "Top 5 words with highest lexical scores:\n",
      "Washington: 500.00\n",
      "Korean: 180.00\n",
      "flown: 140.00\n",
      "Sunday: 120.00\n",
      "falls: 95.83\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "\n",
    "# Sample sentence\n",
    "# sentence = \"This is a complicated sentence with various intricate words and powerful emotions.\"\n",
    "story = random_story()\n",
    "sentence = \"\"\n",
    "\n",
    "for sent in story:\n",
    "    sentence += sent + \" \"\n",
    "\n",
    "print(sentence)\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Calculate word frequency using NLTK's FreqDist\n",
    "word_freq = FreqDist(words)\n",
    "\n",
    "corpus = nltk.corpus.brown.words()  # Replace with your own corpus if available\n",
    "corpus_freq = FreqDist(corpus)\n",
    "\n",
    "# Function to calculate lexical score based on word frequency\n",
    "def calculate_lexical_score(word):\n",
    "    word = word.lower()\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if synsets:\n",
    "        # Consider the number of synsets (senses) for the word\n",
    "        return len(synsets) * 10 * len(word) / (corpus_freq[word] + 1)\n",
    "    else:\n",
    "        # Assign a low score to words not found in WordNet\n",
    "        return 0  # You can adjust this as needed\n",
    "\n",
    "# Calculate lexical scores for each word in the sentence\n",
    "lexical_scores = {word: calculate_lexical_score(word) for word in words}\n",
    "\n",
    "# Print the lexical scores\n",
    "for word, score in lexical_scores.items():\n",
    "    print(f\"{word}: {score:.2f}\")\n",
    "\n",
    "# Print top 5 words with highest lexical scores\n",
    "print(\"\\nTop 5 words with highest lexical scores:\")\n",
    "for word, score in sorted(lexical_scores.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"{word}: {score:.2f}\")\n",
    "\n",
    "# function that returns sorted lexical scores\n",
    "def get_lexical_scores(story):\n",
    "    sentence = \"\"\n",
    "\n",
    "    for sent in story:\n",
    "        sentence += sent + \" \"\n",
    "\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    # remove duplicate words in sentence\n",
    "    words = list(dict.fromkeys(words))\n",
    "\n",
    "    lexical_scores = {word: calculate_lexical_score(word) for word in words}\n",
    "    return sorted(lexical_scores.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "\n",
    "# Initialize the SyllableTokenizer\n",
    "tokenizer = SyllableTokenizer()\n",
    "\n",
    "def get_syllables(word):\n",
    "    # Use the SyllableTokenizer to break the word into syllables\n",
    "    syllables = tokenizer.tokenize(word)\n",
    "\n",
    "    # Join the syllables with a dot (·)\n",
    "    syllables_with_dots = ' · '.join(syllables)\n",
    "\n",
    "    # Print the word with syllables separated by dots\n",
    "    return syllables_with_dots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "nltk.download('cmudict')\n",
    "\n",
    "def get_sounds(text):\n",
    "    d = cmudict.dict()\n",
    "\n",
    "    phonetics = d[text.lower()]\n",
    "\n",
    "    sounds = [sound[:-1] if sound[-1] in \"0123\" else sound for sound in phonetics[0]]\n",
    "    line = \" \".join(sounds)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essays/file-64.txt\n",
      "Fires are destructive. They also create changes. In 1871 a fire burned most of Chicago to the ground. Many people think the fire was started when a cow knocked over a gas lamp. Most of the city at the time was built of wood. This, combined with the famous Chicago winds and a drought, made the city burn down quickly. The fire lasted for three days. 100,000 people were left homeless, and at least 300 were killed. What was amazing was how quickly the city was rebuilt, eventually becoming the third most populous city in the United States. The Great Boston Fire of 1872 created a property damage of $73.5 million,  more than any other fire in the U.S. history. Most of Downtown Boston and the financial district burned down in the fire that began in a warehouse basement. 30 people died, and thousands lost their jobs and their homes. However, the city was rebuilt in two years. It began enforcing building regulations because of the fire. The 1911 Triangle Shirtwaist Factory fire in New York City caused the fourth highest loss of life from an industrial accident in the U.S. history. 123 women and 23 men, all garment workers and mostly poor immigrants, died from the fire, smoke inhalation, or falling or jumping to their deaths. Child labor was not uncommon. The youngest victims were two 14-year-old girls who worked  in the factory. So many people died because the owners of the factory locked the doors to the stairs and exits. This was to prevent the workers from taking breaks and stealing. As a result of the fire and huge loss of life, many people began protesting poor working conditions. The U.S. Congress passed laws improving factory safety conditions. Women workers also formed a union to fight for better working conditions. \n",
      "Infinitive Verbs: ['prevent', 'fight']\n",
      "Past Tense Verbs: ['burned', 'was', 'was', 'made', 'lasted', 'were', 'were', 'was', 'was', 'was', 'created', 'burned', 'began', 'died', 'lost', 'was', 'began', 'caused', 'died', 'was', 'were', 'worked', 'died', 'locked', 'was', 'began', 'passed', 'formed']\n",
      "Present Participle Verbs (Gerunds): ['amazing', 'becoming', 'enforcing', 'falling', 'jumping', 'taking', 'stealing', 'protesting', 'improving']\n",
      "Past Participle Verbs: ['started', 'knocked', 'built', 'combined', 'left', 'killed', 'rebuilt', 'rebuilt']\n",
      "Present Simple Verbs (3rd person singular and present participle): ['are', 'create', 'think', 'burn']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sentence containing verbs with different tenses\n",
    "story = random_story()\n",
    "sentence = \" \".join(story)\n",
    "print(sentence)\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Perform part-of-speech tagging\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "# Initialize lists to categorize verbs by tense\n",
    "infinitive_verbs = []\n",
    "past_tense_verbs = []\n",
    "present_participle_verbs = []\n",
    "past_participle_verbs = []\n",
    "present_simple_verbs = []\n",
    "\n",
    "# Iterate through the tagged words to categorize verbs by tense\n",
    "for word, tag in pos_tags:\n",
    "    if tag == \"VB\":\n",
    "        infinitive_verbs.append(word)\n",
    "    elif tag == \"VBD\":\n",
    "        past_tense_verbs.append(word)\n",
    "    elif tag == \"VBG\":\n",
    "        present_participle_verbs.append(word)\n",
    "    elif tag == \"VBN\":\n",
    "        past_participle_verbs.append(word)\n",
    "    elif tag == \"VBP\" or tag == \"VBZ\":\n",
    "        present_simple_verbs.append(word)\n",
    "\n",
    "# Print the categorized verbs\n",
    "print(\"Infinitive Verbs:\", infinitive_verbs)\n",
    "print(\"Past Tense Verbs:\", past_tense_verbs)\n",
    "print(\"Present Participle Verbs (Gerunds):\", present_participle_verbs)\n",
    "print(\"Past Participle Verbs:\", past_participle_verbs)\n",
    "print(\"Present Simple Verbs (3rd person singular and present participle):\", present_simple_verbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couple/file-45.txt\n",
      "\tVocabulary Worksheet\n",
      "\n",
      "1. pensive\t\t\ta. operate a dial to select a telephone number\n",
      "2. sunday\t\t\tb. device for converting sound waves into electrical energy\n",
      "3. picking\t\t\tc. a dark region of considerable extent on the surface of the moon\n",
      "4. cans\t\t\td. deeply or seriously thoughtful\n",
      "5. maria\t\t\te. as cold as ice\n",
      "6. mike\t\t\tf. yielding readily to pressure or weight\n",
      "7. ice-cold\t\t\tg. deliver a sharp blow or push :\n",
      "8. cleaners\t\t\th. 1/60 of a minute; the basic unit of time adopted under the Systeme International d'Unites\n",
      "9. dialed\t\t\ti. first day of the week; observed as a day of rest and worship by most Christians\n",
      "10. seconds\t\t\tj. underpants worn by women\n",
      "11. pants\t\t\tk. the quantity of a crop that is harvested\n",
      "12. hang\t\t\tl. a special way of doing something\n",
      "13. invited\t\t\tm. pat or squeeze fondly or playfully, especially under the chin\n",
      "14. grabbed\t\t\tn. airtight sealed metal container for food or drink or paint etc.\n",
      "15. cheerfully\t\t\to. for a short time\n",
      "16. couch\t\t\tp. take hold of so as to seize or restrain or stop the motion of\n",
      "17. patted\t\t\tq. shop where dry cleaning is done\n",
      "18. shortly\t\t\tr. increase the likelihood of\n",
      "19. knocked\t\t\ts. in a cheerful manner\n",
      "20. soft\t\t\tt. an upholstered seat for more than one person\n",
      "\n",
      "\n",
      "\tAnswer Key\n",
      "\n",
      "1. pensive - deeply or seriously thoughtful\n",
      "2. sunday - first day of the week; observed as a day of rest and worship by most Christians\n",
      "3. picking - the quantity of a crop that is harvested\n",
      "4. cans - airtight sealed metal container for food or drink or paint etc.\n",
      "5. maria - a dark region of considerable extent on the surface of the moon\n",
      "6. mike - device for converting sound waves into electrical energy\n",
      "7. ice-cold - as cold as ice\n",
      "8. cleaners - shop where dry cleaning is done\n",
      "9. dialed - operate a dial to select a telephone number\n",
      "10. seconds - 1/60 of a minute; the basic unit of time adopted under the Systeme International d'Unites\n",
      "11. pants - underpants worn by women\n",
      "12. hang - a special way of doing something\n",
      "13. invited - increase the likelihood of\n",
      "14. grabbed - take hold of so as to seize or restrain or stop the motion of\n",
      "15. cheerfully - in a cheerful manner\n",
      "16. couch - an upholstered seat for more than one person\n",
      "17. patted - pat or squeeze fondly or playfully, especially under the chin\n",
      "18. shortly - for a short time\n",
      "19. knocked - deliver a sharp blow or push :\n",
      "20. soft - yielding readily to pressure or weight\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# get a random story and create a function to get the 10 hardest words in it and create a worksheet with those words and their definitions\n",
    "story = random_story()\n",
    "hardest_words = [word[0] for word in get_lexical_scores(story)[:20]]\n",
    "definitions = [wordnet.synsets(word)[0].definition() for word in hardest_words]\n",
    "\n",
    "# remove duplicate definitions and words\n",
    "for definition in definitions:\n",
    "    if definitions.count(definition) > 1:\n",
    "        index = definitions.index(definition)\n",
    "        definitions.pop(index)\n",
    "        hardest_words.pop(index)\n",
    "\n",
    "answers = \"\"\n",
    "\n",
    "for index, word in enumerate(hardest_words):\n",
    "    answers += f\"{index + 1}. {word} - {definitions[index]}\\n\"\n",
    "\n",
    "shuffle(definitions)\n",
    "\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\".lower()\n",
    "\n",
    "worksheet = \"\\tVocabulary Worksheet\\n\\n\"\n",
    "for index, word in enumerate(hardest_words):\n",
    "    worksheet += f\"{index + 1}. {word}\\t\\t\\t{alphabet[index]}. {definitions[index]}\\n\"\n",
    "\n",
    "worksheet += \"\\n\\n\\tAnswer Key\\n\\n\"\n",
    "worksheet += answers\n",
    "print(worksheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('siblings', 80.0),\n",
       " ('keys', 47.05882352941177),\n",
       " ('TV', 40.0),\n",
       " ('clean', 23.846153846153847),\n",
       " ('drove', 19.841269841269842),\n",
       " ('drive', 18.085106382978722),\n",
       " ('offered', 10.833333333333334),\n",
       " ('I', 10.0),\n",
       " ('complained', 8.695652173913043),\n",
       " ('watching', 8.533333333333333)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
